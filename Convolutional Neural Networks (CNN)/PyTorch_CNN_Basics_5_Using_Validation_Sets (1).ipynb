{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch CNN Basics 5 - Using Validation Sets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2o-pY2hxmKx"
      },
      "source": [
        "In PyTorch, we usually split a dataset into train, validation and test sets for many reasons. One is to avoid overfitting (high variance). We usually achieve this by doing random splitting on the dataset. In PyTorch, it can be done using SubsetRandomSampler object. As a demo, I am going to split the training part of MNIST dataset into training and validation. After randomly shuffling the dataset, I use the first 55000 points for training, and the remaining 5000 points for validation.\n",
        "\n",
        "* Validation sets are usually the ones used to test for overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds-3hOJux5L8"
      },
      "source": [
        "# Shuffle the indices\n",
        "indices = np.arange(60000)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Build the train loader\n",
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
        "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
        "\n",
        "# Build the validation loader\n",
        "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU1m3oEmzKYo"
      },
      "source": [
        "In the code above, I basically generate a numpy array containing the indices of the photos in the MNIST dataset then randomly shuffle them and then build a train and validation loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHpdN1tWzJxO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}